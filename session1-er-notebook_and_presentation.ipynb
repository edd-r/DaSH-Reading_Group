{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DaSH Reading Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First one, don't know how this is going to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Don't know much about neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Never tried live coding before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So let's do both at the same time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![no idea](noideadog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are neural networks?\n",
    "\n",
    "Discuss...\n",
    "\n",
    "what is an...\n",
    "- input layer\n",
    "- hidden layer\n",
    "- output layer/unit\n",
    "- cost function\n",
    "- activation function\n",
    "\n",
    "what do you think NN are good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boolean logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is \n",
    "- AND\n",
    "- OR\n",
    "- XOR?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1,1],[0, 1, 0, 1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_and = X[:, 0] & X[:, 1]\n",
    "y_or = X[:, 0] | X[:, 1]\n",
    "y_xor = X[:, 0] ^ X[:, 1]\n",
    "\n",
    "print(f\"AND: {y_and}\")\n",
    "print(f\"OR:  {y_or}\")\n",
    "print(f\"XOR: {y_xor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pretty much a linear model, so its output $y$ is \n",
    "$$y = X^Tw+b$$\n",
    "\n",
    "Where... \n",
    "- $y$ is target variable array\n",
    "- $X$ is input data matrix\n",
    "- $w$ is weights array\n",
    "- $b$ is bias or intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many ways of finding $w$ and $b$ this with python; sklearn, statsmodels etc. to keep things simple we use numpy since that is what I have imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Obviously when I say simple I mean difficult for ourselves, because that is when we learn things!\n",
    "\n",
    "We can find the weights w using this equation\n",
    "\n",
    "$$w = (X^TX)^{-1}X^Ty$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DIY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "build your own linear unit class with numpy!\n",
    "\n",
    "method ```calc_output``` implements $y = X^Tw+b$\n",
    "\n",
    "method ```calc_params``` implements $w = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Useful things to know in numpy...\n",
    "- $X^T$ is ```X.T```\n",
    "- $Xy$ is ```X.dot(y)```\n",
    "- The inverse of $X^TX$ can be found using ```np.linalg.inv()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "if you add a dummy variable of ones to X like so...\n",
    "\n",
    "``` dummies = np.ones(X.shape[0],1)\n",
    "    X_b = np.append(dummies, X)\n",
    "```\n",
    "\n",
    "passing X_b to the implementation of the normal equation gives you an extra parameter, which is the bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1: Make your very own linear unit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class LiU:\n",
    "    \n",
    "    # we can pass weights if we have pre-trained them\n",
    "    def __init__(self, weights=None, bias=None):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.y = None\n",
    "        self.weighted_input = None\n",
    "\n",
    "    def calc_output(self, X):\n",
    "        # linear equation with its params\n",
    "        # Xw\n",
    "        self.weighted_input = # fill in\n",
    "        # Xw+b\n",
    "        self.y = # fill in\n",
    "        \n",
    "    def calc_params(self, X, y):\n",
    "        \n",
    "        # get dummies for calculating bias\n",
    "        dummies = np.ones((X.shape[0],1))\n",
    "        X_b = np.append(dummies, X, axis=1)\n",
    "        \n",
    "        # normal equation implemenation\n",
    "        #XT_X\n",
    "        XT_X = X_b.T.dot(X_b)# fill in\n",
    "        #inverse\n",
    "        XT_X_inverse = # fill in\n",
    "        #inverse_XT_y to get \n",
    "        params = # fill in\n",
    "        \n",
    "        \n",
    "        # seperate weights\n",
    "        self.bias = params[0]\n",
    "        self.weights = params[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear unit with boolean problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "and_LiU = LiU()\n",
    "and_LiU.calc_params(X, y_and)\n",
    "and_LiU.calc_output(X)\n",
    "print(f\"linear unit AND: {and_LiU.y}\\nexpected AND:{y_and}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "or_LiU = LiU()\n",
    "or_LiU.calc_params(X,y_or)\n",
    "or_LiU.calc_output(X)\n",
    "print(f\"linear unit OR: {or_LiU.y}\\nexpected OR:{y_or}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xor_LiU = LiU()\n",
    "xor_LiU.calc_params(X,y_xor)\n",
    "xor_LiU.calc_output(X)\n",
    "print(f\"linear unit XOR: {xor_LiU.y}\\nexpected XOR:{y_xor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How did we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cost function, what's that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mean Squared Error\n",
    "\n",
    "Very common way of evalutation ML models across all observations.\n",
    "Basically take the difference between the target and predictions, square it (so all errors are positive and bigger errors are made bigger-er) then average across all observations\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum^{n}_{i=1} (y_i-\\hat{y}_i)^2 $$\n",
    "\n",
    "How to implement?\n",
    "```np.square()```, ```np mean()``` and ```np.subtract()```work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises 2 - 5: Your own cost functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_hat, y):\n",
    "    \"\"\"\n",
    "    gets mean squared error\n",
    "    Args:\n",
    "        y_hat: predictions\n",
    "        y: actual \n",
    "    \"\"\"\n",
    "    # get square of the errors\n",
    "    squared_errors = # fill in\n",
    "    # get mean of the above\n",
    "    mse = # fill in\n",
    "    \n",
    "    return mse, squared_errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mean Absolute Error\n",
    "Similar to mean squared error, except instead of squaring the error, we take the absolute (value without sign) error. So we take the postive difference between predicted and target \n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum^{n}_{i=1} abs(y_i-\\hat{y}_i) $$\n",
    "\n",
    "```np.absolute()``` works here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_hat, y):\n",
    "    \"\"\"\n",
    "    gets mean squared error\n",
    "    Args:\n",
    "        y_hat: predictions\n",
    "        y: actual\n",
    "    \"\"\"\n",
    "    \n",
    "    n = y.size\n",
    "    abs_errors = # fill in\n",
    "    mas = # fill in\n",
    "    return mas, abs_errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross entropy\n",
    "\n",
    "Entropy is a measure of uncertainty. Cross entropy is a measure of uncertainty between two distributions.\n",
    "\n",
    "We can compute the uncertainty between a model's target variable predictions based on the observations, that are known, and the unknown true distribution of the target variable and the observations.\n",
    "\n",
    "We can compute the cross entropy $J$ using the probability that a given observation $x$ belongs to category $y$ according to a trained model as\n",
    "\n",
    "$$ J = \\sum^{n}_{i=1}\\sum^{M}_{c=1}-log(p(y_c|x_i))$$\n",
    "\n",
    "where:\n",
    "- $M$: Number of classes we are predicting in our data\n",
    "- $c$: Specific class\n",
    "- $n$: Number of observations in dataset\n",
    "- $i$: Specific observation\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a binary problem $(M=2)$ this can be written as \n",
    "\n",
    "$$ J = -\\frac{1}{n}\\sum^N_{i=1}y_ilog(p(y))+(1-y)log(1-p(y_i))$$\n",
    "\n",
    "This can be implemented as the following for a single data point\n",
    "```python\n",
    "\n",
    "if y==1:\n",
    "    -np.log(p)\n",
    "else:\n",
    "    -np.log(1-p)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def binary_log_loss(y_prob, y_target):\n",
    "    \n",
    "    losses = [ # fill in if target==1\n",
    "              else # fill in \n",
    "              for p, target in zip(y_prob, y_target)\n",
    "             ]\n",
    "    \n",
    "    return sum(losses), losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can examine the difference between cost functions by computing the costs for each for a target of p=1 for the correct class for an observation across a range of predicted values from 0 to 1 for that class from an (imaginary) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = np.linspace(0,1)\n",
    "y_target = np.ones(y_hat.shape)\n",
    "ll, losses = binary_log_loss(y_hat, y_target)\n",
    "mse, squared_errors = mean_squared_error(y_hat,y_target)\n",
    "mas, abs_errors = mean_absolute_error(y_hat,y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_hat,losses)\n",
    "plt.plot(y_hat,squared_errors )\n",
    "plt.plot(y_hat, abs_errors)\n",
    "plt.xlabel(\"predicted proability\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.title(\"squared error with target of 1\")\n",
    "plt.legend([\"log loss\", \"squared error\", \"absolute error\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that our binary log-loss penalises poor estimates more harshly, leading to a greater graidient.\n",
    "\n",
    "While MSE and MAS might be good for evaluating models, log loss is better with gradient descent training algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating a linear unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "and_loss,_ = binary_log_loss(and_LiU.y, y_and)\n",
    "and_mse,_ = mean_squared_error(and_LiU.y, y_and)\n",
    "and_mas, _ = mean_absolute_error(and_LiU.y, y_and)\n",
    "print(f\"linear unit AND: {and_LiU.y}\\nexpected AND:{y_and}\\nlog-loss: {and_loss}\\nMSE: {and_mse} \\nMAS: {and_mas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_loss,_ = binary_log_loss(or_LiU.y, y_or)\n",
    "or_mse,_ = mean_squared_error(or_LiU.y, y_or)\n",
    "or_mas, _ = mean_absolute_error(or_LiU.y, y_or)\n",
    "print(f\"linear unit or: {or_LiU.y}\\nexpected or:{y_or}\\nlog-loss: {or_loss}\\nMSE: {or_mse} \\nMAS: {or_mas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "not bad, if we say we can round, that works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xor_loss,_ = binary_log_loss(xor_LiU.y, y_xor)\n",
    "xor_mse,_ = mean_squared_error(xor_LiU.y, y_xor)\n",
    "xor_mas, _ = mean_absolute_error(xor_LiU.y, y_xor)\n",
    "print(f\"linear unit xor: {xor_LiU.y}\\nexpected xor:{y_xor}\\nlog-loss: {xor_loss}\\nMSE: {xor_mse} \\nMAS: {xor_mas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ok, that is bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why doesn't it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "lets plot things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_logic(X, \n",
    "               y, \n",
    "               w, \n",
    "               b, \n",
    "               title=None, \n",
    "               seperator=True, \n",
    "               auto_bb=False,\n",
    "               x_label=\"X1\",\n",
    "               y_label=\"X0\"):\n",
    "    \n",
    "    # create plots\n",
    "    fig, ax = plt.subplots()\n",
    "    # expand limits out so we can see everythinga\n",
    "    if not auto_bb:\n",
    "        ax.set_xlim(left=-0.2, right=1.2)\n",
    "        ax.set_ylim(bottom=-0.2, top=1.2)\n",
    "    \n",
    "    # titles etc.\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    \n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlabel(x_label)\n",
    "    \n",
    "    # plot the linear decision boudary\n",
    "    if seperator:\n",
    "        # get cordinates\n",
    "        x0 = np.linspace(-0.2, 2.2, 11)\n",
    "        x1 = x0\n",
    "        x0_cords, x1_cords = np.meshgrid(x0, x1)\n",
    "\n",
    "        # put into array for computing coutour values\n",
    "        X_values = np.array([[x0, x1]\n",
    "                             for x0, x1 in it.product(x0, x1)\n",
    "                             ])\n",
    "\n",
    "        # get values using linear model and passed weight and bias\n",
    "        contours = X_values.dot(w)+b\n",
    "        contours = contours.reshape(x0.shape[0], x1.shape[0])\n",
    "        # plot contours\n",
    "        labels = ax.contour(x0_cords,\n",
    "                            x1_cords,\n",
    "                            contours,\n",
    "                            levels=[0.5])\n",
    "        labels.levels = np.array([\"decision boundary\"])\n",
    "        ax.clabel(labels)\n",
    "\n",
    "    # plot the binary values\n",
    "    ax.scatter(X[:, 1], X[:, 0], c=\"k\", marker=\"x\")\n",
    "    for x0_label, x1_label, y in zip(X[:, 0], X[:, 1], y):\n",
    "        ax.text(x1_label,x0_label, y, fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_logic(X,y_and,and_LiU.weights,and_LiU.bias, title=\"AND operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "if we interpret that as a decision boundary we've train a binary linear classifier for AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_logic(X,y_or,or_LiU.weights, or_LiU.bias, title=\"OR operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Very similar. Note; the only thing that changes is the bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"and weights: {and_LiU.weights} - or weights: {or_LiU.weights}\")\n",
    "print(f\"and bias: {and_LiU.bias} - or bias: {or_LiU.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_logic(X,y_xor,xor_LiU.weights, xor_LiU.bias, title=\"XOR operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the warning is from the contour function in matplotlib, here is means it can't draw a decision boundary.\n",
    "We see that XOR is not linearly seperable so we can't solve it with our linear unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rectifier unit\n",
    "\n",
    "A rectifier unit does what neural networks do a lot of the time. Bring some non-linearity to the problem. The rectifier unit applies a rectifier function to the output of the linear model. When we do this, this is called the activation function. So the output is.\n",
    "\n",
    "$$ y = max(X^Tw+b, 0)$$\n",
    "\n",
    "All it is doing is setting any output that would be  less than 0 as 0.\n",
    "we can use ```np.maximum()``` to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 6 - Rectifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_rect = np.linspace(-2,2)\n",
    "y_rect = # fill in\n",
    "plt.plot(x_rect,y_rect)\n",
    "plt.xlabel(\"input (x_rect)\")\n",
    "plt.ylabel(\"output (y_rect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Easy, now lets implement this. It is very similar to the LiU class, except we are not going to train the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 7 - Rectifier Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.y = None\n",
    "        self.pre_trans_y = None\n",
    "        self.weighted_input = None\n",
    "\n",
    "    def calc_output(self, X):\n",
    "        # linear equation with its params\n",
    "        self.weighted_input = X.dot(self.weights)\n",
    "        self.pre_trans_y = self.weighted_input + self.bias\n",
    "        \n",
    "        # final rectifed output\n",
    "        self.y = np.maximum(self.pre_trans_y, 0) # fill in\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is not really enough to solve XOR, we need to add in another part of neural networks. The hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is one of the reasons neural networks are so powerful. Hidden layers create task relevant abstractions of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "\n",
    "    def __init__(self, weight_matrix, bias_array, unit_type=ReLU):\n",
    "\n",
    "        # create units in this layer\n",
    "        self.units = [unit_type(weights=ws, bias=bs)\n",
    "                      for ws, bs in zip(weight_matrix, bias_array)\n",
    "                      ]\n",
    "\n",
    "        # our output values are blank for now\n",
    "        self.weighted_input = None\n",
    "        self.pre_trans_output = None\n",
    "        self.output = None\n",
    "\n",
    "    def calc_output(self, X):\n",
    "\n",
    "        # calculate each unit's output\n",
    "        for unit in self.units:\n",
    "            unit.calc_output(X)\n",
    "\n",
    "        # store these computed outputs\n",
    "        self.weighted_input = np.array([unit.weighted_input\n",
    "                                        for unit in self.units])\n",
    "\n",
    "        self.pre_trans_output = np.array([unit.pre_trans_y\n",
    "                                          for unit in self.units])\n",
    "\n",
    "        self.output = np.array([unit.y\n",
    "                                for unit in self.units])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are not going to train hidden layers or our rectifier unit, so we steal the weights from the book when we create them.\n",
    "Backpropagation is used for this type of thing, which will *probably* be covered in a couple of sessions time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W_hidden = np.array([[1,1],[1,1]])\n",
    "c = np.array([0, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving our boolean problems... nearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With these, lets build a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ReLU_layer = HiddenLayer(W_hidden, c)\n",
    "ReLU_layer.calc_output(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And check these..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# XW or 6.8 in book\n",
    "ReLU_layer.weighted_input.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 6.9 in book\n",
    "ReLU_layer.pre_trans_output.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "the final output, compare this to the target xor output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# h matrix 6.10 in book\n",
    "ReLU_layer.output.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is this doing? What did we say hidden layers do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_logic(X,y_xor,xor_LiU.weights, xor_LiU.bias, title=\"XOR operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_logic(ReLU_layer.output.T,\n",
    "           y_xor,\n",
    "           xor_LiU.weights,  # i have to put something here\n",
    "           xor_LiU.bias,  # it is just the way the function is written\n",
    "           title=\"hidden layer output operation\",\n",
    "           seperator=False,\n",
    "           auto_bb=True,\n",
    "           y_label=\"h0\",\n",
    "           x_label=\"h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can you see what it is doing yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finally solving XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "lets use the hidden layer output with our linear xor solver that failed so badly last time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xor_LiU.calc_params(ReLU_layer.output.T, y_xor)\n",
    "xor_LiU.calc_output(ReLU_layer.output.T)\n",
    "\n",
    "xor_loss,_ = binary_log_loss(xor_LiU.y, y_xor)\n",
    "xor_mse,_ = mean_squared_error(xor_LiU.y, y_xor)\n",
    "xor_mas, _ = mean_absolute_error(xor_LiU.y, y_xor)\n",
    "print(f\"linear unit xor: {xor_LiU.y}\\nexpected xor:{y_xor}\\nlog-loss: {xor_loss}\\nMSE: {xor_mse} \\nMAS: {xor_mas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why this works\n",
    "\n",
    "lets have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_logic(ReLU_layer.output.T,\n",
    "           y_xor,\n",
    "           xor_LiU.weights,\n",
    "           xor_LiU.bias, \n",
    "           title=\"hidden layer output operation\",\n",
    "           seperator=True,\n",
    "           auto_bb=True,\n",
    "           y_label=\"h0\",\n",
    "           x_label=\"h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The problem is now linearly seperable, so we can solve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- May have bitten off more than I can chew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![toobig](sticktobugdog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I hope you have learnt something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![someidea](someideadog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Happy?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "reading_group",
   "language": "python",
   "name": "reading_group"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
